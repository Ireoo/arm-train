# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations
import random

import torch
from typing import TYPE_CHECKING

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils.math import wrap_to_pi
from isaaclab.envs.mdp.events import reset_root_state_uniform

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def joint_pos_target_l2(env: ManagerBasedRLEnv, target: float, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """Penalize joint position deviation from a target value."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # wrap the joint positions to (-pi, pi)
    joint_pos = wrap_to_pi(asset.data.joint_pos[:, asset_cfg.joint_ids])
    # compute the reward
    return torch.sum(torch.square(joint_pos - target), dim=1)


def end_effector_position_l2(env: ManagerBasedRLEnv, target_position: list, asset_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """Penalize end-effector position deviation from a target 3D position."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get end-effector position - shape: (num_envs, 3)
    ee_pos = asset.data.body_pos_w[:, asset.find_bodies(body_name)[0], :3]
    
    # Convert target position to tensor with correct shape (num_envs, 3)
    # Use the actual batch size from ee_pos
    num_envs = ee_pos.shape[0]
    target_pos = torch.tensor(target_position, dtype=torch.float32, device=env.device)
    target_pos = target_pos.unsqueeze(0).expand(num_envs, -1)  # Shape: (num_envs, 3)
    
    # compute the L2 distance to target - shape: (num_envs,)
    distance = torch.norm(ee_pos - target_pos, dim=1)
    return distance


def end_effector_position_to_marker_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """Penalize end-effector position deviation from target marker position."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position - shape: (num_envs, 3)
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            print(f"Warning: Body '{body_name}' not found, using root position")
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            # Remove extra dimensions if present: (num_envs, 1, 3) -> (num_envs, 3)
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        print(f"Error finding body '{body_name}': {e}")
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target marker position - shape: (num_envs, 3)
    target_pos = target_marker.data.root_pos_w[:, :3]
    
    # Ensure both tensors have the correct shape
    if ee_pos.shape[0] != num_envs or ee_pos.shape[1] != 3:
        print(f"Error: ee_pos has wrong shape {ee_pos.shape}, expected ({num_envs}, 3)")
        return torch.zeros(num_envs, device=env.device, dtype=torch.float32)
    
    if target_pos.shape[0] != num_envs or target_pos.shape[1] != 3:
        print(f"Error: target_pos has wrong shape {target_pos.shape}, expected ({num_envs}, 3)")
        return torch.zeros(num_envs, device=env.device, dtype=torch.float32)
    
    # compute the L2 distance to target - shape: (num_envs,)
    try:
        distance = torch.norm(ee_pos - target_pos, dim=1)
        
        # Final safety check
        if distance.shape[0] != num_envs:
            print(f"Error: distance shape {distance.shape} != expected ({num_envs},)")
            return torch.zeros(num_envs, device=env.device, dtype=torch.float32)
              
        return distance
        
    except Exception as e:
        print(f"Error computing distance: {e}")
        return torch.zeros(num_envs, device=env.device, dtype=torch.float32)


def target_reached_bonus(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """ç»™äºˆæˆåŠŸåˆ°è¾¾ç›®æ ‡çš„å¥–åŠ±åŠ æˆã€‚"""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position - shape: (num_envs, 3)
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target marker position - shape: (num_envs, 3)
    target_pos = target_marker.data.root_pos_w[:, :3]
    
    # compute the L2 distance to target - shape: (num_envs,)
    distance = torch.norm(ee_pos - target_pos, dim=1)
    
    # è¯¾ç¨‹å­¦ä¹ ï¼šåŠ¨æ€è°ƒæ•´æˆåŠŸé˜ˆå€¼ï¼Œä»å®¹æ˜“åˆ°å›°éš¾
    if hasattr(env, '_curriculum_step'):
        env._curriculum_step += 1
    else:
        env._curriculum_step = 0
    
    # æ¸è¿›å¼æˆåŠŸé˜ˆå€¼ï¼šä»8cm->5cm->3cm->2cm
    if env._curriculum_step < 20000:
        success_threshold = 0.08  # å‰20kæ­¥ï¼š8cm
    elif env._curriculum_step < 40000:
        success_threshold = 0.05  # 20-40kæ­¥ï¼š5cm  
    elif env._curriculum_step < 60000:
        success_threshold = 0.03  # 40-60kæ­¥ï¼š3cm
    else:
        success_threshold = 0.02  # 60k+æ­¥ï¼š2cm
    
    success_bonus = torch.where(distance < success_threshold, 
                               torch.tensor(300.0, device=env.device, dtype=torch.float32),  # å¢åŠ æˆåŠŸå¥–åŠ±
                               torch.tensor(0.0, device=env.device, dtype=torch.float32))
    
    collision_detected = distance < success_threshold
        
    # å¦‚æœæ£€æµ‹åˆ°ç¢°æ’ï¼Œä»…æ‰“å°è°ƒè¯•ä¿¡æ¯
    if torch.any(collision_detected):
        collision_count = torch.sum(collision_detected).item()
        collision_indices = torch.where(collision_detected)[0]
        if len(collision_indices) > 0:
            sample_idx = collision_indices[0].item()
            # sample_ee_pos = ee_pos[sample_idx].cpu().numpy()
            # sample_target_pos = target_pos[sample_idx].cpu().numpy()
            sample_distance = distance[sample_idx].item()
            # print(f"ç›®æ ‡åˆ°è¾¾: æ£€æµ‹åˆ°{collision_count}ä¸ªæˆåŠŸåˆ°è¾¾äº‹ä»¶, å¥–åŠ±{success_bonus[sample_idx]:.4f}, æ ·æœ¬æˆåŠŸ #{sample_idx}: è·ç¦»={sample_distance:.4f}m")
            update_target_marker(env, asset_cfg, target_cfg, collision_indices)
    
    return success_bonus


def distance_guidance_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """åŸºäºè·ç¦»çš„å¼•å¯¼å¥–åŠ±ï¼Œè·ç¦»è¶Šè¿‘å¥–åŠ±è¶Šé«˜ã€‚"""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position - shape: (num_envs, 3)
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target marker position - shape: (num_envs, 3)
    target_pos = target_marker.data.root_pos_w[:, :3]
    
    # compute the L2 distance to target - shape: (num_envs,)
    distance = torch.norm(ee_pos - target_pos, dim=1)
    
    # åŸºäºè·ç¦»çš„æ¸è¿›å¥–åŠ±ï¼šè·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜
    # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°ï¼šreward = exp(-distance * scale)
    guidance_reward = torch.exp(-distance * 3.0)  # é™ä½scaleä½¿è¿œè·ç¦»ä¹Ÿæœ‰å¥–åŠ±
    
    # æ·»åŠ åæ‡’æƒ°æœºåˆ¶ï¼šå¥–åŠ±å‘ç›®æ ‡ç§»åŠ¨çš„è¡Œä¸º
    if hasattr(env, '_prev_distance'):
        # å¦‚æœè·ç¦»åœ¨ç¼©çŸ­ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
        distance_improvement = env._prev_distance - distance
        improvement_bonus = torch.clamp(distance_improvement * 100.0, -5.0, 10.0)  # é™åˆ¶å¥–åŠ±èŒƒå›´
        guidance_reward += improvement_bonus
    
    env._prev_distance = distance.clone()
    
    return guidance_reward


def approach_progress_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """å¥–åŠ±æœºæ¢°è‡‚æ¥è¿‘ç›®æ ‡çš„è¿›æ­¥ã€‚"""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position - shape: (num_envs, 3)
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target marker position - shape: (num_envs, 3)
    target_pos = target_marker.data.root_pos_w[:, :3]
    
    # compute the L2 distance to target - shape: (num_envs,)
    distance = torch.norm(ee_pos - target_pos, dim=1)
    
    # åˆ›å»ºä¸€ä¸ªæ›´æ¿€è¿›çš„æ¥è¿‘å¥–åŠ±
    # å½“è·ç¦»å°äºæŸä¸ªé˜ˆå€¼æ—¶ç»™äºˆå¤§å¥–åŠ±
    close_threshold = 0.1  # 10cmå†…ç»™äºˆæ¥è¿‘å¥–åŠ±
    very_close_threshold = 0.05  # 5cmå†…ç»™äºˆæ›´é«˜å¥–åŠ±
    
    approach_reward = torch.zeros_like(distance)
    
    # 10cmå†…ç»™äºˆåŸºç¡€æ¥è¿‘å¥–åŠ±
    close_mask = distance < close_threshold
    approach_reward[close_mask] += 10.0
    
    # 5cmå†…ç»™äºˆé¢å¤–å¥–åŠ±
    very_close_mask = distance < very_close_threshold
    approach_reward[very_close_mask] += 20.0
    
    # 2cmå†…ç»™äºˆå·¨å¤§å¥–åŠ±
    success_mask = distance < 0.02
    approach_reward[success_mask] += 50.0
    
    return approach_reward


def convergence_monitor(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """ç›‘æ§è®­ç»ƒæ”¶æ•›çŠ¶æ€çš„å…³é”®æŒ‡æ ‡ã€‚"""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target position
    target_pos = target_marker.data.root_pos_w[:, :3]
    
    # Calculate distances
    distance = torch.norm(ee_pos - target_pos, dim=1)
    
    # æ”¶æ•›æŒ‡æ ‡ç»Ÿè®¡
    success_count = torch.sum(distance < 0.05).item()  # 5cmå†…æˆåŠŸæ•°
    close_count = torch.sum(distance < 0.1).item()    # 10cmå†…æ¥è¿‘æ•°
    avg_distance = torch.mean(distance).item()         # å¹³å‡è·ç¦»
    
    # æ¯100æ­¥æ‰“å°ä¸€æ¬¡æ”¶æ•›ç»Ÿè®¡
    if hasattr(env, '_convergence_step_counter'):
        env._convergence_step_counter += 1
    else:
        env._convergence_step_counter = 0
    
    if env._convergence_step_counter % 1000 == 0:
        success_rate = success_count / num_envs * 100
        close_rate = close_count / num_envs * 100
        
        print(f"=== æ”¶æ•›ç›‘æ§ (Step {env._convergence_step_counter}) ===")
        print(f"æˆåŠŸç‡ (5cmå†…): {success_rate:.1f}% ({success_count}/{num_envs})")
        print(f"æ¥è¿‘ç‡ (10cmå†…): {close_rate:.1f}% ({close_count}/{num_envs})")
        print(f"å¹³å‡è·ç¦»: {avg_distance:.4f}m")
        
        # æ”¶æ•›åˆ¤æ–­é€»è¾‘
        if success_rate >= 80:
            print("ğŸ‰ æ”¶æ•›åˆ¤æ–­: ä»»åŠ¡æˆåŠŸç‡è¾¾æ ‡ (>=80%)")
        elif success_rate >= 50:
            print("ğŸ”„ æ”¶æ•›åˆ¤æ–­: æ¥è¿‘æ”¶æ•› (50-80%)")
        elif close_rate >= 30:
            print("ğŸ“ˆ æ”¶æ•›åˆ¤æ–­: å­¦ä¹ è¿›å±•è‰¯å¥½ (>30%æ¥è¿‘)")
        else:
            print("ğŸš€ æ”¶æ•›åˆ¤æ–­: ç»§ç»­è®­ç»ƒä¸­...")
        
        print("=" * 50)
    
    return torch.zeros(num_envs, device=env.device, dtype=torch.float32)


def termination_monitor(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """ç›‘æ§ç»ˆæ­¢æ¡ä»¶ï¼Œå¸®åŠ©è°ƒè¯•ã€‚"""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # æ£€æŸ¥å…³èŠ‚ä½ç½®æ˜¯å¦è¶…å‡ºè¾¹ç•Œ
    joint_pos = asset.data.joint_pos
    joint_bounds_violations = 0
    
    # æ£€æŸ¥ä¸»è¦å…³èŠ‚ï¼ˆjoint_2-7ï¼‰
    for i in range(1, 7):  # joint_2 to joint_7 (0-indexed: 1-6)
        if i < joint_pos.shape[1]:
            joint_i_pos = joint_pos[:, i]
            out_of_bounds = torch.logical_or(joint_i_pos < -3.0 * torch.pi, joint_i_pos > 3.0 * torch.pi)
            if torch.any(out_of_bounds):
                violations = torch.sum(out_of_bounds).item()
                joint_bounds_violations += violations
                if violations > 0:
                    print(f"å…³èŠ‚ {i+1} è¾¹ç•Œè¿è§„: {violations} ä¸ªç¯å¢ƒ")
    
    # æ£€æŸ¥æœ«ç«¯æ‰§è¡Œå™¨å…³èŠ‚ï¼ˆjoint_1, joint_8ï¼‰
    for i in [0, 7]:  # joint_1 and joint_8 (0-indexed: 0, 7)
        if i < joint_pos.shape[1]:
            joint_i_pos = joint_pos[:, i]
            out_of_bounds = torch.logical_or(joint_i_pos < -torch.pi, joint_i_pos > 3.0 * torch.pi)
            if torch.any(out_of_bounds):
                violations = torch.sum(out_of_bounds).item()
                joint_bounds_violations += violations
                if violations > 0:
                    print(f"æœ«ç«¯å…³èŠ‚ {i+1} è¾¹ç•Œè¿è§„: {violations} ä¸ªç¯å¢ƒ")
    
    if joint_bounds_violations > 0:
        print(f"æ€»è®¡å…³èŠ‚è¾¹ç•Œè¿è§„: {joint_bounds_violations} æ¬¡")
    
    return torch.zeros(num_envs, device=env.device, dtype=torch.float32)


def update_target_marker(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, collision_indices):
    # å®šä¹‰ä½ç½®èŒƒå›´å’Œé€Ÿåº¦èŒƒå›´
    pose_range = {
        "x": (-0.3, 0.3),  # xè½´èŒƒå›´: åœ†å¿ƒÂ±89cm (è€ƒè™‘89cmæœ€å¤§åŠå¾„)
        "y": (-0.3, 0.3),  # yè½´èŒƒå›´: åœ†å¿ƒÂ±89cm
        "z": (0.1, 0.3),   # zè½´èŒƒå›´: åŸºå‡†ä½ç½®å‘ä¸Š15cmé™„è¿‘
        "roll": (0.0, 0.0),  # ä¿æŒæ—‹è½¬ä¸º0
        "pitch": (0.0, 0.0),
        "yaw": (0.0, 0.0),
    }
    velocity_range = {
        "x": (0.0, 0.0),
        "y": (0.0, 0.0),
        "z": (0.0, 0.0),
        "roll": (0.0, 0.0),
        "pitch": (0.0, 0.0),
        "yaw": (0.0, 0.0),
    }
    
    # æ­£ç¡®è°ƒç”¨reset_root_state_uniformå‡½æ•°
    # æ ¹æ®é”™è¯¯ä¿¡æ¯ï¼Œtarget_cfgä¸æ˜¯æœ‰æ•ˆçš„ç´¢å¼•å‚æ•°
    # éœ€è¦æ‰¾åˆ°æ­£ç¡®çš„ç›®æ ‡å¯¹è±¡ç´¢å¼•æˆ–IDæ¥é‡ç½®çŠ¶æ€
    try:
        # ç›´æ¥ä½¿ç”¨target_cfgæ¥è·å–ç›®æ ‡å¯¹è±¡å¹¶é‡ç½®å…¶çŠ¶æ€
        # target_cfgæ˜¯SceneEntityCfgå¯¹è±¡ï¼ŒåŒ…å«äº†ç›®æ ‡æ ‡è®°çš„ä¿¡æ¯
        reset_root_state_uniform(
            env, 
            env_ids=collision_indices,  # åªé‡ç½®ç¢°æ’çš„ç¯å¢ƒ
            asset_cfg=target_cfg,  # ä½¿ç”¨ä¼ å…¥çš„ç›®æ ‡é…ç½®
            pose_range=pose_range, 
            velocity_range=velocity_range
        )
        # print(f"æˆåŠŸé‡ç½®äº† {len(collision_indices)} ä¸ªç¯å¢ƒçš„ç›®æ ‡ä½ç½®")
    except Exception as e:
        print(f"é‡ç½®ç›®æ ‡ä½ç½®æ—¶å‡ºé”™: {e}")
        print(f"target_cfgç±»å‹: {type(target_cfg)}, target_cfgåç§°: {target_cfg.name if hasattr(target_cfg, 'name') else 'æœªçŸ¥'}")
    # target_cfg.init_state.pos = (random.uniform(-0.89, 0.89), random.uniform(-0.89, 0.89), random.uniform(0.1, 0.5))


def joint_velocity_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """å¥–åŠ±å…³èŠ‚è¿åŠ¨ï¼Œé˜²æ­¢æ‡’æƒ°è¡Œä¸ºã€‚"""
    asset: Articulation = env.scene[asset_cfg.name]
    
    # è·å–å…³èŠ‚é€Ÿåº¦
    joint_vel = asset.data.joint_vel
    
    # è®¡ç®—å…³èŠ‚é€Ÿåº¦çš„L2èŒƒæ•°ï¼ˆæ€»è¿åŠ¨é‡ï¼‰
    velocity_magnitude = torch.norm(joint_vel, dim=1)
    
    # å¥–åŠ±é€‚åº¦çš„è¿åŠ¨ï¼Œä½†æƒ©ç½šè¿‡åº¦çš„è¿åŠ¨
    # ç›®æ ‡æ˜¯é¼“åŠ±æœ‰ç›®çš„çš„è¿åŠ¨è€Œä¸æ˜¯éšæœºæŠ–åŠ¨
    optimal_velocity = 2.0  # rad/sï¼Œé€‚åº¦çš„å…³èŠ‚è¿åŠ¨é€Ÿåº¦
    velocity_reward = torch.exp(-torch.abs(velocity_magnitude - optimal_velocity) / optimal_velocity)
    
    # åŸºç¡€è¿åŠ¨å¥–åŠ±ï¼šåªè¦åœ¨åŠ¨å°±ç»™å¥–åŠ±
    movement_bonus = torch.clamp(velocity_magnitude * 0.5, 0.0, 2.0)
    
    return velocity_reward + movement_bonus


def exploration_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """å¥–åŠ±æ¢ç´¢æ–°åŒºåŸŸï¼Œé˜²æ­¢é™·å…¥å±€éƒ¨åŒºåŸŸã€‚"""
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # ç»´æŠ¤æ¢ç´¢å†å²ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    if not hasattr(env, '_exploration_history'):
        env._exploration_history = []
        env._exploration_step = 0
    
    env._exploration_step += 1
    
    # æ¯100æ­¥è®°å½•ä¸€æ¬¡ä½ç½®
    if env._exploration_step % 100 == 0:
        current_positions = ee_pos.cpu().numpy()
        env._exploration_history.append(current_positions)
        
        # åªä¿ç•™æœ€è¿‘çš„50ä¸ªè®°å½•
        if len(env._exploration_history) > 50:
            env._exploration_history.pop(0)
    
    # è®¡ç®—æ¢ç´¢å¥–åŠ±ï¼šä¸å†å²ä½ç½®çš„å¹³å‡è·ç¦»
    exploration_reward = torch.zeros(num_envs, device=env.device, dtype=torch.float32)
    
    if len(env._exploration_history) > 1:
        for i, historical_pos in enumerate(env._exploration_history[-10:]):  # æ£€æŸ¥æœ€è¿‘10ä¸ªå†å²ä½ç½®
            historical_tensor = torch.tensor(historical_pos, device=env.device, dtype=torch.float32)
            distances = torch.norm(ee_pos - historical_tensor, dim=1)
            # å¥–åŠ±ä¸å†å²ä½ç½®çš„è·ç¦»ï¼ˆé¼“åŠ±æ¢ç´¢æ–°åŒºåŸŸï¼‰
            exploration_reward += torch.clamp(distances * 2.0, 0.0, 5.0)
        
        exploration_reward /= min(len(env._exploration_history), 10)  # å¹³å‡åŒ–
    
    return exploration_reward


def anti_stagnation_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_cfg: SceneEntityCfg, body_name: str = "arm_end") -> torch.Tensor:
    """ååœæ»å¥–åŠ±ï¼šæ£€æµ‹å¹¶æƒ©ç½šé•¿æ—¶é—´ä¸æ”¹å–„çš„è¡Œä¸ºã€‚"""
    asset: Articulation = env.scene[asset_cfg.name]
    target_marker = env.scene[target_cfg.name]
    
    # Get the actual number of environments
    num_envs = asset.data.root_pos_w.shape[0]
    
    # Get end-effector position
    try:
        body_indices = asset.find_bodies(body_name)
        if len(body_indices) == 0:
            ee_pos = asset.data.root_pos_w[:, :3]
        else:
            ee_pos = asset.data.body_pos_w[:, body_indices[0], :3]
            if ee_pos.dim() == 3 and ee_pos.shape[1] == 1:
                ee_pos = ee_pos.squeeze(1)
    except Exception as e:
        ee_pos = asset.data.root_pos_w[:, :3]
    
    # Get target position
    target_pos = target_marker.data.root_pos_w[:, :3]
    distance = torch.norm(ee_pos - target_pos, dim=1)
    
    # ç»´æŠ¤æ€§èƒ½å†å²
    if not hasattr(env, '_performance_history'):
        env._performance_history = []
        env._stagnation_counter = torch.zeros(num_envs, device=env.device)
    
    # æ¯50æ­¥æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æ”¹å–„
    if len(env._performance_history) == 0:
        env._performance_history.append(distance.clone())
        return torch.zeros(num_envs, device=env.device, dtype=torch.float32)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
    last_distance = env._performance_history[-1]
    improvement = last_distance - distance  # æ­£å€¼è¡¨ç¤ºæ”¹å–„
    
    # æ›´æ–°åœæ»è®¡æ•°å™¨
    no_improvement_mask = improvement < 0.001  # æ”¹å–„å°äº1mmè§†ä¸ºæ²¡æœ‰æ”¹å–„
    env._stagnation_counter[no_improvement_mask] += 1
    env._stagnation_counter[~no_improvement_mask] = 0  # æœ‰æ”¹å–„åˆ™é‡ç½®è®¡æ•°å™¨
    
    # è®¡ç®—ååœæ»å¥–åŠ±
    anti_stagnation_reward = torch.zeros(num_envs, device=env.device, dtype=torch.float32)
    
    # æƒ©ç½šé•¿æ—¶é—´åœæ»ï¼ˆè¶…è¿‡500æ­¥æ²¡æœ‰æ”¹å–„ï¼‰
    stagnation_penalty = torch.where(env._stagnation_counter > 500,
                                   -torch.log(env._stagnation_counter / 500.0),  # éšæ—¶é—´å¢åŠ çš„æƒ©ç½š
                                   torch.tensor(0.0, device=env.device))
    
    # å¥–åŠ±æœ€è¿‘çš„æ”¹å–„
    recent_improvement_bonus = torch.clamp(improvement * 50.0, -2.0, 5.0)
    
    anti_stagnation_reward = recent_improvement_bonus + stagnation_penalty
    
    # æ›´æ–°å†å²ï¼ˆä¿ç•™æœ€è¿‘çš„è®°å½•ï¼‰
    env._performance_history.append(distance.clone())
    if len(env._performance_history) > 100:  # åªä¿ç•™æœ€è¿‘100ä¸ªè®°å½•
        env._performance_history.pop(0)
    
    return anti_stagnation_reward